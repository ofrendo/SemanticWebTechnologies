\section{Known limitations}
To begin with, one limitation of the application is that it is restricted to English texts. This is because CoreNLP's English models are used by default. One solution to this would be to first use language detection on any input and to then use the corresponding model. Other languages such as German and Spanish are available for download\footnote{\url{http://nlp.stanford.edu/software/CRF-NER.shtml}}. 

Another limitation depends on the input. If the input contains no named entities then it is impossible for the application to search for additional information. Conversely, the input can contain a large number of entities. In this case the waiting time may be very high, or the server may run out of memory. An improvement to both of these problems would be to cache commonly request entites locally, for example in Jena's TBD\footnote{\url{https://jena.apache.org/documentation/tdb/}}.  

Additionally, the input can contain ambiguous entities. One possibility where this happens is when named entities such as ``Mr. \texttt{\{surname\}}'' are found in the input. In this case many subjects with the surname may be found and the application must rely on scoring URIs as described in section \ref{sec:sparqlQueries}. 

Lastly, the secure server (nginx, as described in section \ref{sec:server}) uses a self signed certificate. Given more time the server would be hosted on the internet instead of running it locally. A domain could then be set up, for which a secure certificate could be bought or generated. 




Sascha?: We could mention papers describing techniques which could improve certain areas like LINDA \cite{boehm_linda:_2012} for entity matching. 


